{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10485,
     "status": "ok",
     "timestamp": 1719524830453,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "k_LyWafwh9rl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1719524860259,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "y5DRGYi1dYl1",
    "outputId": "9e7a0db7-26a3-42e8-b527-b29beb0aad2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1719524864461,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "jURr_iNOg5vy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1719524869169,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "8r99SdEeJgBK",
    "outputId": "868930fa-c201-4467-f9af-e451fc5ba942"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pham\\AppData\\Local\\Temp\\ipykernel_6100\\1351635241.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVECTNwlmydy"
   },
   "source": [
    "The next cell is used in case of real time detection, it needs to access camera for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1719524910049,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "tw9xDOfvKHta",
    "outputId": "4962e74d-096e-41c4-f52a-205d780b6d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "# checking run time\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "else:\n",
    "    device_name = torch.device('cpu')\n",
    "print(\"Using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrtG6n_0B_sY"
   },
   "source": [
    "# Data preparation pipepline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1719524990070,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "XeaUFfbgkr7O"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719524994754,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "_XpHcCBbZJUb"
   },
   "outputs": [],
   "source": [
    "def get_video_paths_and_labels(root_dir):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(root_dir))\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "\n",
    "    print(f\"class names: {class_names}\")\n",
    "    print(f\"mapping index to class names: {class_to_idx} \")\n",
    "    for cls_name in class_names:\n",
    "        cls_dir = os.path.join(root_dir, cls_name)\n",
    "        print(f\"Checking directory: {cls_dir}\")\n",
    "        if os.path.isdir(cls_dir):  # Check if it's a directory\n",
    "            for filename in os.listdir(cls_dir):\n",
    "                if filename.endswith(('.mp4', '.avi', '.mov')):\n",
    "                    video_path = os.path.join(cls_dir, filename)\n",
    "                    video_paths.append(video_path)\n",
    "                    labels.append(class_to_idx[cls_name])\n",
    "                    print(f\"Found video: {video_path}, label: {class_to_idx[cls_name]}\")\n",
    "                else:\n",
    "                    print(f\"Skipping non-video file: {filename}\")\n",
    "\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8LG2Ysfi5LV"
   },
   "source": [
    "Process video into frames, pack with frames, labels, and keypoints lists so run the GetKps first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1719525000475,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "auJP3qfus4a0"
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, model_path=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.kp_extractor = GetKps(model_path) if model_path else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        keypoints_list = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Extract keypoints if the keypoint extractor is available\n",
    "            if self.kp_extractor:\n",
    "                kps = self.kp_extractor.extractkps(np.array(frame.permute(1, 2, 0)))\n",
    "                keypoints_list.append(kps)\n",
    "\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return frames, keypoints_list, label\n",
    "\n",
    "class GetKps():\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path).to(device_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def extractkps(self, frame):\n",
    "        results = self.model(frame)\n",
    "        kps_list = []\n",
    "\n",
    "        if results:\n",
    "            for r in results:\n",
    "                if hasattr(r, 'keypoints'):\n",
    "                    keypoints = r.keypoints.xyn.cpu().numpy()\n",
    "                    kps_list.append(keypoints)\n",
    "                else:\n",
    "                    # print(\"No keypoints attribute found in the results.\")\n",
    "                    kps_list.append(self._get_dummy_keypoints(frame.shape))\n",
    "        else:\n",
    "            # print(\"No results found.\")\n",
    "            kps_list.append(self._get_dummy_keypoints(frame.shape))\n",
    "        return kps_list\n",
    "    def _get_dummy_keypoints(self, shape):\n",
    "        dumy_keypoints = np.zeros((17, 3))\n",
    "        return dumy_keypoints\n",
    "\n",
    "\n",
    "\n",
    "# Define the model path\n",
    "model_path = \"best.pt\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1719525006863,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "IuVhrh-QZX0S"
   },
   "outputs": [],
   "source": [
    "# Define the transforms\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1719525012007,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "7bkUy9skZfKl",
    "outputId": "a471d11b-07a6-4976-b99f-3b53008ffacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['lame', 'sound']\n",
      "mapping index to class names: {'lame': 0, 'sound': 1} \n",
      "Checking directory: dataset/train\\lame\n",
      "Found video: dataset/train\\lame\\1.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\2.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\3.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\4.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\5.mp4, label: 0\n",
      "Skipping non-video file: download.jpg\n",
      "Skipping non-video file: horse-4062214.webp\n",
      "Skipping non-video file: lame-horse-1.webp\n",
      "Skipping non-video file: tt-blog-3.jpg\n",
      "Checking directory: dataset/train\\sound\n",
      "Found video: dataset/train\\sound\\2122952-hd_1280_720_60fps.mp4, label: 1\n",
      "Found video: dataset/train\\sound\\2865004-hd_1280_720_30fps.mp4, label: 1\n",
      "Found video: dataset/train\\sound\\2865027-hd_1280_720_30fps.mp4, label: 1\n",
      "Skipping non-video file: Copy of 11-Black-and-White-Horse-Breeds-with-_jpg.rf.bf221ecf541b08703e4bb3b4fc504d29(1).jpg\n",
      "Skipping non-video file: Copy of 11-Black-and-White-Horse-Breeds-with-_jpg.rf.bf221ecf541b08703e4bb3b4fc504d29.jpg\n",
      "Skipping non-video file: Copy of 128-584-Black-Horse-Stock-Photos-Free-1-_jpg.rf.6c08389bb0cbb991a5e2dd927bd79893.jpg\n",
      "Skipping non-video file: Copy of 265-Dark-Brown-Horse-Running-Fast-Stock-_jpg.rf.456e46f255b3544cbbf14fb1f808117c.jpg\n",
      "Skipping non-video file: Copy of 33-Ranning-ideas-in-2024-_-beautiful-_jpg.rf.97d4ea775ec89ee819fee214aa8154f3.jpg\n",
      "Skipping non-video file: Copy of 427-564-Black-Horse-Royalty-Free-Images-_jpg.rf.7d840f2141d7018526f2c8ac2c066a4d.jpg\n",
      "Skipping non-video file: Copy of 54-000-Black-Horse-Stock-Photos-1-_jpg.rf.876d4de86bbcd764be6a2d5f9513ae9e.jpg\n"
     ]
    }
   ],
   "source": [
    "# Get video paths and labels\n",
    "train_path = 'dataset/train'\n",
    "video_paths, labels = get_video_paths_and_labels(train_path)\n",
    "train_dataset = VideoDataset(video_paths, labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23258,
     "status": "error",
     "timestamp": 1719357719684,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "bKwg-Pc_gU0q",
    "outputId": "7325573d-1b2c-4a3c-a3eb-e19f56733d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: torch.Size([1, 3, 208, 128, 128])\n",
      "Keypoints list: []\n",
      "Label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Example usage: iterating over the DataLoader\n",
    "for frames, keypoints_list, label in train_loader:\n",
    "    print(f\"Frames shape: {frames.shape}\")\n",
    "    print(f\"Keypoints list: {keypoints_list}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDT4qxSlY8wp"
   },
   "source": [
    "**Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9472,
     "status": "ok",
     "timestamp": 1719525046070,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "BlWvJrMynvg-",
    "outputId": "132598c5-2f0b-4818-f39a-a844c49e35f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3D_2L(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv3D_2L(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3D_2L, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "                        \n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "model_load_path = 'lame horse'\n",
    "feature_model = torch.load(model_load_path)\n",
    "feature_model.fc = nn.Identity()\n",
    "feature_model = feature_model.to(device_name)\n",
    "feature_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1719526902795,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "Fn2u7HCsqzXF"
   },
   "outputs": [],
   "source": [
    "class HorseLamenessClassifier(nn.Module):\n",
    "    def __init__(self, feature_extraction, features_dim, keypoints_dim):\n",
    "        super().__init__()\n",
    "        self.feature = feature_extraction\n",
    "        self.keypoints_pool = nn.AdaptiveAvgPool2d((1,keypoints_dim))\n",
    "        self.classifier = nn.Linear(features_dim + keypoints_dim, 2)\n",
    "\n",
    "    def forward(self, video_frames, keypoints=None):\n",
    "        features = self.feature(video_frames)\n",
    "        # features = features.mean(dim=1)\n",
    "        if keypoints is not None and len(keypoints) > 0:\n",
    "            keypoints = np.array(keypoints)\n",
    "            keypoints = torch.from_numpy(np.concatenate(keypoints, axis=0)).float().to(device_name)                                           \n",
    "            keypoints = self.keypoints_pool(keypoints).squeeze(0)\n",
    "            # keypoints = torch.from_numpy(keypoints).float().to(device_name) \n",
    "            combined_features = torch.cat((features, keypoints), dim=1)  \n",
    "        else: combined_features = features\n",
    "        # combined_features = combined_features.mean(dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\pham\\anaconda3\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['lame', 'sound']\n",
      "mapping index to class names: {'lame': 0, 'sound': 1} \n",
      "Checking directory: dataset/val\\lame\n",
      "Found video: dataset/val\\lame\\2.mp4, label: 0\n",
      "Checking directory: dataset/val\\sound\n",
      "Found video: dataset/val\\sound\\2122952-hd_1280_720_60fps.mp4, label: 1\n"
     ]
    }
   ],
   "source": [
    "val_path = 'dataset/val'\n",
    "val_video_paths, val_labels = get_video_paths_and_labels(val_path)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_dataset = VideoDataset(val_video_paths, val_labels, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "df32aa1c5f004ee3b5741b3b52d2418f",
      "633047dd73b8494ba566dfed1ab9ed02",
      "2f037f24dfd44abf8523571a70096502",
      "0f59b32ff9e746039e2bc2a1b56d162a",
      "ca9d888dde1f464aaf97d9d1d5687c4e",
      "38b0e1fc41344592972780c0970aa87a",
      "4450e88a2ac046ea9dea48d93312486e",
      "d0434ad46acc4a9d8b8fbb5466444dd8",
      "0c66febdc3544b118dc6fc2d536c4451",
      "2b4bdf3c875a430e8dee3c3649e75078",
      "4bdb691c71d240658cf6dec5a777c161"
     ]
    },
    "id": "tKy_drKZT5c_",
    "outputId": "7aa735df-4496-4864-9e28-c8b5c166ba1f"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 83x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keypoints)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(videos,keypoints)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: outputs \u001b[38;5;241m=\u001b[39m model(videos)\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mHorseLamenessClassifier.forward\u001b[1;34m(self, video_frames, keypoints)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: combined_features \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# combined_features = combined_features.mean(dim=1)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(combined_features)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 83x2)"
     ]
    }
   ],
   "source": [
    "model = HorseLamenessClassifier(feature_extraction = feature_model,features_dim= 32, keypoints_dim = 17*3 )\n",
    "model = model.to(device_name)\n",
    "criterion = nn.CrossEntropyLoss().to(device_name)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for videos, keypoints, labels in train_loader:\n",
    "        videos = videos.to(device_name)\n",
    "        keypoints = [kp.to(device_name) for kp in keypoints]\n",
    "        labels = labels.to(device_name)\n",
    "        optimizer.zero_grad()\n",
    "        if len(keypoints)>0:\n",
    "            outputs = model(videos,keypoints)\n",
    "        else: outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, keypoints, labels in val_loader:\n",
    "            videos = videos.to(device_name)\n",
    "            keypoints = [kp.to(device_name) for kp in keypoints]\n",
    "            labels = labels.to(device_name)\n",
    "            if len(keypoints)>0:\n",
    "                outputs = model(videos,keypoints)\n",
    "            else: outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = 'simpleclassifier_model'  # Create a directory to store the model\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# 1. Save the state dictionary (weights only)\n",
    "state_dict_path = os.path.join(model_save_dir, 'model_state_dict.pth')\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(f\"Model state dictionary saved to {state_dict_path}\")\n",
    "\n",
    "# 2. Save the entire model (architecture + weights)\n",
    "entire_model_path = os.path.join(model_save_dir, 'entire_model.pth')\n",
    "torch.save(model, entire_model_path)\n",
    "print(f\"Entire model saved to {entire_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTn-zCnFzvfi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    # Print GPU memory stats\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check if GPU is enabled in your Colab runtime.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvPNyMVng0w7"
   },
   "outputs": [],
   "source": [
    "print('Predicted class:', torch.argmax(output, dim=1).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzl5xx4ezvEW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNBkkG9rNifG0r85+f7F8/f",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1avwwsZOMEdiGdVRja21gaGt6kDveeFk9",
     "timestamp": 1719476916557
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c66febdc3544b118dc6fc2d536c4451": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f59b32ff9e746039e2bc2a1b56d162a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b4bdf3c875a430e8dee3c3649e75078",
      "placeholder": "​",
      "style": "IPY_MODEL_4bdb691c71d240658cf6dec5a777c161",
      "value": " 0/8 [00:00&lt;?, ?it/s]"
     }
    },
    "2b4bdf3c875a430e8dee3c3649e75078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f037f24dfd44abf8523571a70096502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0434ad46acc4a9d8b8fbb5466444dd8",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c66febdc3544b118dc6fc2d536c4451",
      "value": 0
     }
    },
    "38b0e1fc41344592972780c0970aa87a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4450e88a2ac046ea9dea48d93312486e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bdb691c71d240658cf6dec5a777c161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "633047dd73b8494ba566dfed1ab9ed02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38b0e1fc41344592972780c0970aa87a",
      "placeholder": "​",
      "style": "IPY_MODEL_4450e88a2ac046ea9dea48d93312486e",
      "value": "  0%"
     }
    },
    "ca9d888dde1f464aaf97d9d1d5687c4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0434ad46acc4a9d8b8fbb5466444dd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df32aa1c5f004ee3b5741b3b52d2418f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_633047dd73b8494ba566dfed1ab9ed02",
       "IPY_MODEL_2f037f24dfd44abf8523571a70096502",
       "IPY_MODEL_0f59b32ff9e746039e2bc2a1b56d162a"
      ],
      "layout": "IPY_MODEL_ca9d888dde1f464aaf97d9d1d5687c4e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
