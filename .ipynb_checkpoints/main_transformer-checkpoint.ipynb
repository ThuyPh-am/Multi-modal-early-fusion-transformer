{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k_LyWafwh9rl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\pham\\anaconda3\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1719441405437,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "y5DRGYi1dYl1",
    "outputId": "18b354a7-1180-4d3b-c8a3-2a802d036da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jURr_iNOg5vy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1486,
     "status": "ok",
     "timestamp": 1719441428726,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "8r99SdEeJgBK",
    "outputId": "45b19ecc-4dec-4b27-d2f0-707dfdcab3bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pham\\AppData\\Local\\Temp\\ipykernel_2684\\1351635241.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVECTNwlmydy"
   },
   "source": [
    "The next cell is used in case of real time detection, it needs to access camera for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1719441472549,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "tw9xDOfvKHta",
    "outputId": "3a2635eb-faab-4691-df1e-18ddf0287490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "# checking run time\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device(\"cuda\")\n",
    "else:\n",
    "    device_name = torch.device('cpu')\n",
    "print(\"Using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrtG6n_0B_sY"
   },
   "source": [
    "# Data preparation pipepline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XeaUFfbgkr7O"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_XpHcCBbZJUb"
   },
   "outputs": [],
   "source": [
    "def get_video_paths_and_labels(root_dir):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(root_dir))\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "\n",
    "    print(f\"class names: {class_names}\")\n",
    "    print(f\"mapping index to class names: {class_to_idx} \")\n",
    "    for cls_name in class_names:\n",
    "        cls_dir = os.path.join(root_dir, cls_name)\n",
    "        print(f\"Checking directory: {cls_dir}\")\n",
    "        if os.path.isdir(cls_dir):  # Check if it's a directory\n",
    "            for filename in os.listdir(cls_dir):\n",
    "                if filename.endswith(('.mp4', '.avi', '.mov')):\n",
    "                    video_path = os.path.join(cls_dir, filename)\n",
    "                    video_paths.append(video_path)\n",
    "                    labels.append(class_to_idx[cls_name])\n",
    "                    print(f\"Found video: {video_path}, label: {class_to_idx[cls_name]}\")\n",
    "                else:\n",
    "                    print(f\"Skipping non-video file: {filename}\")\n",
    "\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8LG2Ysfi5LV"
   },
   "source": [
    "Process video into frames, pack with frames, labels, and keypoints lists so run the GetKps first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "auJP3qfus4a0"
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, model_path=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.kp_extractor = GetKps(model_path) if model_path else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        keypoints_list = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Extract keypoints if the keypoint extractor is available\n",
    "            if self.kp_extractor:\n",
    "                kps = self.kp_extractor.extractkps(np.array(frame.permute(1, 2, 0)))\n",
    "                keypoints_list.append(kps)\n",
    "\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return frames, keypoints_list, label\n",
    "\n",
    "class GetKps():\n",
    "    def __init__(self, model_path):\n",
    "        self.model = YOLO(model_path).to(device_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def extractkps(self, frame):\n",
    "        results = self.model(frame)\n",
    "        kps_list = []\n",
    "\n",
    "        if results:\n",
    "            for r in results:\n",
    "                if hasattr(r, 'keypoints'):\n",
    "                    keypoints = r.keypoints.xyn.cpu().numpy()\n",
    "                    kps_list.append(keypoints)\n",
    "                else:\n",
    "                    print(\"No keypoints attribute found in the results.\")\n",
    "                    kps_list.append(self._get_dummy_keypoints(frame.shape))\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "            kps_list.append(self._get_dummy_keypoints(frame.shape))\n",
    "        return kps_list\n",
    "\n",
    "    def _get_dummy_keypoints(self, shape):\n",
    "        dumy_keypoints = np.zeros((17, 3))\n",
    "        return dumy_keypoints\n",
    "\n",
    "\n",
    "\n",
    "# Define the model path\n",
    "model_path = \"best.pt\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IuVhrh-QZX0S"
   },
   "outputs": [],
   "source": [
    "# Define the transforms\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1719442229786,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "7bkUy9skZfKl",
    "outputId": "53ad43ee-5b7a-4511-b2d4-c209097e364d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['lame', 'sound']\n",
      "mapping index to class names: {'lame': 0, 'sound': 1} \n",
      "Checking directory: dataset/train\\lame\n",
      "Found video: dataset/train\\lame\\1.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\2.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\3.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\4.mp4, label: 0\n",
      "Found video: dataset/train\\lame\\5.mp4, label: 0\n",
      "Skipping non-video file: download.jpg\n",
      "Skipping non-video file: horse-4062214.webp\n",
      "Skipping non-video file: lame-horse-1.webp\n",
      "Skipping non-video file: tt-blog-3.jpg\n",
      "Checking directory: dataset/train\\sound\n",
      "Found video: dataset/train\\sound\\2122952-hd_1280_720_60fps.mp4, label: 1\n",
      "Found video: dataset/train\\sound\\2865004-hd_1280_720_30fps.mp4, label: 1\n",
      "Found video: dataset/train\\sound\\2865027-hd_1280_720_30fps.mp4, label: 1\n",
      "Skipping non-video file: Copy of 11-Black-and-White-Horse-Breeds-with-_jpg.rf.bf221ecf541b08703e4bb3b4fc504d29(1).jpg\n",
      "Skipping non-video file: Copy of 11-Black-and-White-Horse-Breeds-with-_jpg.rf.bf221ecf541b08703e4bb3b4fc504d29.jpg\n",
      "Skipping non-video file: Copy of 128-584-Black-Horse-Stock-Photos-Free-1-_jpg.rf.6c08389bb0cbb991a5e2dd927bd79893.jpg\n",
      "Skipping non-video file: Copy of 265-Dark-Brown-Horse-Running-Fast-Stock-_jpg.rf.456e46f255b3544cbbf14fb1f808117c.jpg\n",
      "Skipping non-video file: Copy of 33-Ranning-ideas-in-2024-_-beautiful-_jpg.rf.97d4ea775ec89ee819fee214aa8154f3.jpg\n",
      "Skipping non-video file: Copy of 427-564-Black-Horse-Royalty-Free-Images-_jpg.rf.7d840f2141d7018526f2c8ac2c066a4d.jpg\n",
      "Skipping non-video file: Copy of 54-000-Black-Horse-Stock-Photos-1-_jpg.rf.876d4de86bbcd764be6a2d5f9513ae9e.jpg\n"
     ]
    }
   ],
   "source": [
    "# Get video paths and labels\n",
    "train_path = 'dataset/train'\n",
    "video_paths, labels = get_video_paths_and_labels(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = VideoDataset(video_paths, labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23258,
     "status": "error",
     "timestamp": 1719357719684,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "bKwg-Pc_gU0q",
    "outputId": "7325573d-1b2c-4a3c-a3eb-e19f56733d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: torch.Size([1, 3, 524, 128, 128])\n",
      "Keypoints list: []\n",
      "Label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Example usage: iterating over the DataLoader\n",
    "for frames, keypoints_list, label in train_loader:\n",
    "    print(f\"Frames shape: {frames.shape}\")\n",
    "    print(f\"Keypoints list: {keypoints_list}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDT4qxSlY8wp"
   },
   "source": [
    "**Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9644,
     "status": "ok",
     "timestamp": 1719442257910,
     "user": {
      "displayName": "Thuy Pham",
      "userId": "04564873524139770457"
     },
     "user_tz": -120
    },
    "id": "BlWvJrMynvg-",
    "outputId": "e42d70d0-c7a6-4376-9443-2e0651d24330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3D_2L(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv3D_2L(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3D_2L, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "                        \n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model_load_path = 'lame horse'\n",
    "feature_model = torch.load(model_load_path)\n",
    "feature_model.fc = nn.Identity()\n",
    "feature_model = feature_model.to(device_name)\n",
    "feature_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lNltvPR3Wdvx"
   },
   "outputs": [],
   "source": [
    "# from chatgpt, generated from original paper of transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Fn2u7HCsqzXF"
   },
   "outputs": [],
   "source": [
    "class HorseLamenessClassifier(nn.Module):\n",
    "    def __init__(self, feature_model, d_model, nhead, num_layers):\n",
    "        super(HorseLamenessClassifier,self).__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model) \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.classifier = nn.Linear(d_model, 2)  # 2 classes: lame or sound\n",
    "\n",
    "    def forward(self, video_frames, keypoints=None):\n",
    "        video_features = self.feature_model(video_frames)\n",
    "        if keypoints is not None and len(keypoints)>0:\n",
    "          keypoints = np.array(keypoints)\n",
    "          keypoints = torch.from_numpy(keypoints).float().to(device_name)  \n",
    "          combined_features = torch.cat((video_features, torch.tensor(keypoints).float()), dim= 1)\n",
    "        else:\n",
    "          combined_features = video_features\n",
    "\n",
    "        # Positional Encoding\n",
    "        combined_features = self.positional_encoding(combined_features)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoded_features = self.transformer_encoder(combined_features)\n",
    "        encoded_features = encoded_features.mean(dim=1) # global average pooling over the sequence\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(encoded_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['lame', 'sound']\n",
      "mapping index to class names: {'lame': 0, 'sound': 1} \n",
      "Checking directory: dataset/val\\lame\n",
      "Found video: dataset/val\\lame\\2.mp4, label: 0\n",
      "Checking directory: dataset/val\\sound\n",
      "Found video: dataset/val\\sound\\2122952-hd_1280_720_60fps.mp4, label: 1\n"
     ]
    }
   ],
   "source": [
    "val_path = 'dataset/val'\n",
    "val_video_paths, val_labels = get_video_paths_and_labels(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_dataset = VideoDataset(val_video_paths, val_labels, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames shape: torch.Size([1, 3, 713, 128, 128])\n",
      "Labels shape: torch.Size([1])\n",
      "Frames shape: torch.Size([1, 3, 210, 224, 224])\n",
      "Labels shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "def print_batch_shapes(dataloader):\n",
    "    batch = next(iter(dataloader))\n",
    "    frames, keypoints_list, labels = batch\n",
    "    print(f\"Frames shape: {frames.shape}\")  # Shape of frames tensor\n",
    "    print(f\"Labels shape: {labels.shape}\")  # Shape of labels tensor\n",
    "print_batch_shapes(train_loader)\n",
    "print_batch_shapes(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseLamenessClassifier(feature_model = feature_model, d_model=16, nhead=8, num_layers=4)\n",
    "model = model.to(device_name)\n",
    "criterion = nn.CrossEntropyLoss().to(device_name)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for videos, keypoints, labels in train_loader:\n",
    "        videos = videos.to(device_name)\n",
    "        keypoints = [kp.to(device_name) for kp in keypoints]\n",
    "        labels = labels.to(device_name)\n",
    "        optimizer.zero_grad()\n",
    "        if len(keypoints)>0:\n",
    "            outputs = model(videos,keypoints)\n",
    "        else: outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, keypoints, labels in val_loader:\n",
    "            videos = videos.to(device_name)\n",
    "            keypoints = [kp.to(device_name) for kp in keypoints]\n",
    "            labels = labels.to(device_name)\n",
    "            if len(keypoints)>0:\n",
    "                outputs = model(videos,keypoints)\n",
    "            else: outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1,3,778,128,128)\n",
    "summary(model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = 'lame_horse_model'  # Create a directory to store the model\n",
    "os.makedirs(model_save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# 1. Save the state dictionary (weights only)\n",
    "state_dict_path = os.path.join(model_save_dir, 'model_state_dict.pth')\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(f\"Model state dictionary saved to {state_dict_path}\")\n",
    "\n",
    "# 2. Save the entire model (architecture + weights)\n",
    "entire_model_path = os.path.join(model_save_dir, 'entire_model.pth')\n",
    "torch.save(model, entire_model_path)\n",
    "print(f\"Entire model saved to {entire_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from github.com/chark/movi-dataset-toolkit and github.com/LuizCarlosS/MOT_framework\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_path = 'dataset/test'\n",
    "test_video_paths, test_labels = get_video_paths_and_labels(val_path)\n",
    "test_dataset = VideoDataset(val_video_paths, val_labels, transform=val_transform)\n",
    "test_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "# Load YOLOv8 model\n",
    "model_path = 'best.pt'\n",
    "yolo_model = YOLO(model_path)\n",
    "\n",
    "# Load Pretrained Video Feature Extractor \n",
    "feature_extractor = feature_model()\n",
    "\n",
    "# Initialize the HorseLamenessClassifier model\n",
    "model = HorseLamenessClassifier(feature_model = feature_model, d_model=16, nhead=8, num_layers=4)\n",
    "model.to(device_name)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load and process video\n",
    "video_path = 'test/4.mp4'\n",
    "output_path = 'processed_video_transformer.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract keypoints\n",
    "    results = yolo_model(frame)\n",
    "    keypoints = results[0].keypoints.xy.cpu().numpy() if hasattr(results[0], 'keypoints') else None\n",
    "\n",
    "    # Preprocess frame and get video features (adjust to your model)\n",
    "    processed_frame = preprocess_frame(frame)  # Use your preprocessing function\n",
    "    with torch.no_grad():\n",
    "        video_features = feature_extractor(processed_frame.unsqueeze(0).to(device_name))\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        output = model(video_features, keypoints)  # Get logits\n",
    "        _, predicted = torch.max(output, dim=1)  # Get class prediction\n",
    "\n",
    "    # Draw bounding box and keypoints (same as before)\n",
    "    for result in results:\n",
    "        if hasattr(result, 'boxes'):\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    if keypoints is not None and keypoints.size > 0:\n",
    "        # Extract x and y coordinates (same as before)\n",
    "\n",
    "    # Display Label\n",
    "    label = \"Lame\" if predicted.item() == 1 else \"Sound\"  # Get class label from prediction\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_size = cv2.getTextSize(label, font, 1, 2)[0]\n",
    "    text_x = int((frame.shape[1] - text_size[0]) / 2)\n",
    "    text_y = int((frame.shape[0] + text_size[1]) / 2)\n",
    "    cv2.putText(frame, label, (text_x, text_y), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "   \n",
    "    out.write(frame)  # Write the frame to the output video\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPYhA1sKnJ+eEe6mGrCYmbm",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c1a220e64b40b5845cada94a9bffb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a2ce7ea77d74fa08394313f00aff8be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f1597e5ef1443c3b982d1e92dafa834",
       "IPY_MODEL_c93c1859980b4abf9ae0e9a3540f677c",
       "IPY_MODEL_f09ed92183ea4644b239665ba7a3baf7"
      ],
      "layout": "IPY_MODEL_1509952a268c492fbc211ee075a95299"
     }
    },
    "1509952a268c492fbc211ee075a95299": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f1597e5ef1443c3b982d1e92dafa834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_806148ef7d024f908b1994c23c1e5b65",
      "placeholder": "​",
      "style": "IPY_MODEL_838499c0d1a346fd8b0243f85ca3a55f",
      "value": "  0%"
     }
    },
    "6b46637147d34f629bad939caa95bac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "799aaef4f19744fe99dec09149bc351d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "806148ef7d024f908b1994c23c1e5b65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "838499c0d1a346fd8b0243f85ca3a55f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ff9dd26ace04a1bbc104fdb1edb0a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c93c1859980b4abf9ae0e9a3540f677c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_799aaef4f19744fe99dec09149bc351d",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b46637147d34f629bad939caa95bac3",
      "value": 0
     }
    },
    "f09ed92183ea4644b239665ba7a3baf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07c1a220e64b40b5845cada94a9bffb7",
      "placeholder": "​",
      "style": "IPY_MODEL_9ff9dd26ace04a1bbc104fdb1edb0a37",
      "value": " 0/8 [00:00&lt;?, ?it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
